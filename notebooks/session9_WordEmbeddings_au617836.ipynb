{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll first need to install either the ```spaCY``` medium or large model!\n",
    "\n",
    "->> terminal\n",
    "\n",
    "```cd cds-language\n",
    "source ./lang101/bin/activate\n",
    "python -m spacy download en_core_web_md\n",
    "deactivate```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "# nlp\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # medium model that contains word vectors which the small model does not contain\n",
    "\n",
    "# gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained vectors in ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy comes with pretrained word embeddings generated with word2vec. Each word has a vector representing it. The word embedding models in spaCy are not as accurate as Gensim's models, but they are more simple and intuitive. The spaCy word embeddings are not that useful, because spaCy does not make it clear how they have been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12209  , -0.017989 , -0.046495 , -0.011551 ,  0.56601  ,\n",
       "       -0.41398  ,  0.33022  , -0.33376  , -0.13001  , -0.12592  ,\n",
       "       -0.87791  , -0.23211  ,  0.062879 ,  0.36644  ,  0.054478 ,\n",
       "        0.18169  , -0.17619  ,  0.5006   ,  0.70912  ,  0.072825 ,\n",
       "        0.7663   ,  0.32764  ,  0.32388  , -0.39116  , -0.44868  ,\n",
       "       -0.32976  , -0.076284 , -0.0095968, -0.15763  ,  0.66581  ,\n",
       "       -0.3471   , -0.35091  ,  0.0083347,  0.47103  ,  0.25362  ,\n",
       "        0.33329  ,  0.13503  ,  0.055926 ,  0.30558  , -0.10581  ,\n",
       "        0.0025447, -0.22811  , -0.25086  ,  0.24853  ,  0.041999 ,\n",
       "       -0.45543  ,  0.33007  , -0.63446  ,  0.20003  , -0.16201  ,\n",
       "       -0.73001  ,  0.18834  , -0.08403  , -0.74857  , -0.041885 ,\n",
       "        0.013566 ,  0.12618  , -0.086973 ,  0.6415   , -0.64083  ,\n",
       "       -0.33979  , -0.30045  ,  0.4442   , -0.16814  ,  0.042421 ,\n",
       "        0.38954  ,  0.13112  ,  0.050652 ,  0.028356 , -0.19597  ,\n",
       "       -0.33335  ,  0.51083  ,  0.031252 , -0.46036  ,  0.96367  ,\n",
       "       -0.18269  , -0.45205  ,  0.1219   ,  0.37261  ,  0.21238  ,\n",
       "       -0.066633 ,  0.16912  ,  0.31963  ,  0.38542  ,  0.012271 ,\n",
       "       -0.37636  ,  0.93615  ,  0.099094 ,  0.039206 ,  0.15081  ,\n",
       "       -0.20036  , -0.018291 ,  0.26004  , -0.25271  ,  0.29581  ,\n",
       "        0.25683  , -0.11761  , -0.26145  , -0.18894  ,  0.2304   ,\n",
       "       -0.01222  , -0.34944  , -0.43342  ,  0.53451  ,  0.3406   ,\n",
       "       -1.5429   ,  0.18289  , -0.071331 , -0.41892  , -0.54068  ,\n",
       "        0.57777  , -0.21779  , -0.23462  , -0.23186  ,  0.0097733,\n",
       "        0.0024722,  0.44102  ,  0.42902  , -0.51808  ,  0.27852  ,\n",
       "       -0.55756  ,  0.55691  ,  0.22491  , -0.64264  ,  0.34144  ,\n",
       "        0.81257  ,  0.34056  ,  0.32824  ,  0.15347  , -0.15179  ,\n",
       "        0.0785   , -0.35183  , -0.07151  ,  0.14143  ,  0.076965 ,\n",
       "       -0.19131  , -0.36754  ,  0.22157  , -0.0033847, -0.28939  ,\n",
       "       -0.87098  ,  0.18772  ,  0.38676  , -0.27839  , -0.33864  ,\n",
       "        0.37575  , -0.14742  ,  0.14865  , -0.22118  , -0.2949   ,\n",
       "       -0.075455 , -0.13209  , -0.37975  , -0.57348  ,  0.80542  ,\n",
       "        0.39002  ,  0.10023  ,  0.37752  ,  0.4033   ,  0.5094   ,\n",
       "        0.39902  ,  0.30225  ,  0.0017244,  0.10604  , -0.089823 ,\n",
       "       -0.22495  , -0.19304  ,  0.111    , -0.54012  ,  0.72709  ,\n",
       "       -0.85346  , -0.23112  , -0.12473  ,  0.36965  , -0.16979  ,\n",
       "       -0.22022  ,  0.20447  ,  0.11195  ,  0.12113  , -0.0048103,\n",
       "       -0.022085 ,  0.11868  , -0.21368  ,  0.14229  ,  0.13086  ,\n",
       "        0.39352  ,  0.18102  , -0.0070991, -0.25288  ,  0.053143 ,\n",
       "       -0.32824  ,  0.2129   ,  0.80165  , -0.22614  , -0.58046  ,\n",
       "        0.57305  ,  0.3336   , -0.32476  ,  0.66694  , -0.13694  ,\n",
       "        0.19007  , -0.058861 ,  0.39363  , -0.33488  , -0.06042  ,\n",
       "        0.032613 ,  0.59523  ,  0.33795  ,  0.26685  , -0.3068   ,\n",
       "       -0.048396 ,  0.16648  ,  0.3587   , -0.16531  , -0.4788   ,\n",
       "        0.056003 ,  0.32458  ,  0.071163 , -0.019912 ,  0.64615  ,\n",
       "        0.31968  , -0.10279  ,  0.35027  , -0.70592  ,  0.15582  ,\n",
       "       -0.080219 ,  0.23333  , -0.20954  , -0.25444  ,  0.26664  ,\n",
       "        0.071374 ,  1.1249   , -0.25604  , -0.44826  , -0.16394  ,\n",
       "       -0.24653  ,  0.017129 , -0.32196  ,  0.57451  , -0.095985 ,\n",
       "       -0.073217 ,  0.30681  ,  0.2575   , -0.72925  ,  0.40905  ,\n",
       "       -0.57281  ,  0.52192  ,  0.17846  ,  0.0079109, -0.22521  ,\n",
       "        0.59784  ,  0.072238 ,  0.53683  , -0.77169  ,  0.22214  ,\n",
       "       -0.070167 , -0.21947  , -0.076865 , -0.18089  , -0.40931  ,\n",
       "        0.55222  ,  0.02121  , -0.094077 ,  0.65659  , -0.4784   ,\n",
       "       -0.57449  ,  0.40284  , -0.043305 , -0.84336  ,  0.50217  ,\n",
       "       -0.86302  , -0.070307 , -0.026903 ,  0.59636  , -0.017246 ,\n",
       "        0.41678  ,  0.68058  , -0.26088  , -0.074218 , -0.2563   ,\n",
       "       -0.033267 , -0.23664  , -0.43225  ,  0.14449  ,  0.023437 ,\n",
       "        0.49219  ,  0.09961  ,  0.068301 , -0.33351  ,  0.3648   ,\n",
       "        0.45529  ,  0.46942  ,  0.033431 ,  0.17567  ,  0.44301  ,\n",
       "        0.16516  ,  0.4579   , -0.16739  , -0.40863  ,  0.49869  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the vector for the word \"denmark\"\n",
    "nlp(\"denmark\").vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^This vector is an array of 300 points that encode the word \"denmark\". Each number is a weight that has been calcualted with the word2vec skipgram model. Skipgram trains a logistic classifier on each word, and then the weights that are trained then become this vector for the word. Hence, we have 300 weights/nodes representing a single words. Every word has a unique representation - unique weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comparing individual words__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spacy nlp objects for different words.\n",
    "banana = nlp(\"banana\")\n",
    "apple = nlp(\"apple\")\n",
    "scotland = nlp(\"scotland\")\n",
    "denmark = nlp(\"denmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect word similarities__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spaCy we can investigate the similarity between word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5831844567891399"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana.similarity(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0703526672694443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana.similarity(scotland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Banana and apple are very similar while banana and Scotland are very dissimilar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694890898124977"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denmark.similarity(scotland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Document similarities__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spaCy we can also look at the similarity between sentences/documents. Hence, not just individual words. spaCy does this by taking the vector for each word in the document and then average these vectors, so we can an average vector for each docuemnt. We can then compare these averaged vectors for documents with each other to see how similar documents are to each other. \n",
    "\n",
    "Keep in mind that the results should be taken with a grain of salt, because averaging word vectors is not that informative. Once we have documents consisting of many words, averaging their vectors does not make much sense because it introduces a lot of variance/randomness, and comparing documents in this way is not very useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like bananas\")\n",
    "doc2 = nlp(\"I like apples\")\n",
    "doc3 = nlp(\"I come from Scotland\")\n",
    "doc4 = nlp(\"I live in Denmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6828703253926357"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.similarity(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8041838664871435"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3.similarity(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with ```gensim```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more accurate word embeddings we need to work with the word embeddings provided by gensim. Gensim provides different pretrained word embeddings (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download pretrained models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^These are the different pretrained word embedding models provided by gensim. We can see that there are three different approaches represented: word2vec, GloVe, and fastText. These are different in terms of how they are trained on the data. Their output is essentially the same, but the algorithm used to calcualte the embeddings/weights are not the same across the approaches. FastText models contain subword embeddings which word2vec does not. This means that fastText splits words into subwords which allows for greater generalizability, which allows the model to capture out-of-vocabulary words which is something the word2vec model is not able to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download a pretrained model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.3% 23.4/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.6% 58.4/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.2% 93.8/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the GloVe model that has been trained on the wiki gigaword corpus\n",
    "pretrained_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is useful to train your own word embedding model, but for downstream NLP tasks, e.g. text classification, we can just use a pretrained model that has been trained on a very large corpus to turn a text into a numerical representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect vector for specific word__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28875  , -0.19655  ,  0.26046  ,  0.086723 ,  0.25918  ,\n",
       "       -0.1897   , -0.54331  ,  0.009582 , -0.30836  , -0.0031624,\n",
       "        0.33199  , -0.29428  , -0.24047  ,  1.19     , -0.084937 ,\n",
       "        0.11623  , -0.21052  , -0.54361  , -0.99796  ,  0.12067  ,\n",
       "        0.14138  ,  0.65072  ,  1.2077   ,  1.1735   ,  0.23783  ,\n",
       "       -0.98251  ,  0.41053  ,  0.27652  ,  0.52805  , -0.48693  ,\n",
       "       -0.8589   ,  0.35657  ,  0.71596  ,  0.17604  ,  0.52895  ,\n",
       "       -0.2974   ,  0.44817  ,  0.40725  , -0.98995  , -0.90026  ,\n",
       "       -0.57812  ,  0.050827 ,  0.32352  ,  0.087861 , -0.023458 ,\n",
       "       -0.34776  ,  0.88943  ,  0.10766  ,  0.46515  , -0.20827  ,\n",
       "        0.59546  ,  0.16455  , -0.45227  ,  0.6851   , -0.87772  ,\n",
       "       -1.7848   , -0.37841  , -0.25611  ,  0.15408  ,  0.067509 ,\n",
       "        0.71967  , -0.31071  , -0.15901  , -0.066492 ,  0.50181  ,\n",
       "        0.99762  , -1.1725   ,  1.5181   ,  0.14916  , -0.11483  ,\n",
       "        0.072389 , -0.66993  ,  0.36882  ,  0.37702  ,  0.36758  ,\n",
       "        0.15591  , -0.10071  , -0.53873  , -0.35206  ,  0.60048  ,\n",
       "        0.31707  , -0.47386  ,  0.45003  ,  0.37695  , -0.38389  ,\n",
       "       -0.54477  , -0.28152  , -0.037618 ,  0.20349  , -0.28685  ,\n",
       "        0.083537 , -0.11225  ,  0.74851  , -0.047845 , -0.49077  ,\n",
       "        0.21637  , -0.65435  ,  0.428    ,  0.66858  , -1.0518   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can inspect the vector for the word denmark generated by this particular model\n",
    "pretrained_vectors['denmark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find most similar words to target__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gensim we can find the most similar words to a target word rather than comparing two specific words. This allows us to explore the semantic space of a particular word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sweden', 0.8624401688575745),\n",
       " ('norway', 0.828826367855072),\n",
       " ('netherlands', 0.8032277822494507),\n",
       " ('finland', 0.7628087997436523),\n",
       " ('austria', 0.7483422756195068),\n",
       " ('germany', 0.7414340972900391),\n",
       " ('belgium', 0.7279534935951233),\n",
       " ('hungary', 0.7076718807220459),\n",
       " ('luxembourg', 0.6797298192977905),\n",
       " ('switzerland', 0.6770632266998291)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.most_similar('denmark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most similar words to \"denmark\" are very close in terms of geography. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare specific words__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare two specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61651856"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.similarity('denmark', 'scotland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86244005"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.similarity('denmark', 'sweden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vector algebra__\n",
    "\n",
    "We can use word embeddings to study structural relationships. \n",
    "\n",
    "*Man* is to *king* as *woman* is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.most_similar(positive=['king', 'woman'], \n",
    "                                negative=['man'], \n",
    "                                topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just simple algebra: <br>\n",
    "\n",
    "[V(king) - V(man)] + V(woman)\n",
    "\n",
    "We subract the vector for man from the vector for king and add the vector for women and we get the vector for queen.\n",
    "\n",
    "Here we are working with gender as a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('swimming', 0.6574411988258362)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.most_similar(positive=['walk', 'swim'], \n",
    "                           negative=['walked'], \n",
    "                           topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again it is just simple algebra:\n",
    "\n",
    "\n",
    "[V(walk) - V(walked)] + V(swam)\n",
    "\n",
    "Here we are working with tense as a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('copenhagen', 0.7726544737815857)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.most_similar(positive=['berlin', 'denmark'], \n",
    "                           negative=['germany'], \n",
    "                           topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[V(berlin) - V(germany)] + V(denmark)\n",
    "\n",
    "here we are working with nationality as a dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Odd one out!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a list of words and find the word that does not match - the word that is most dissimilar to the others (furthest away in the embedding space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cds-au617836/cds-language-forked/lang101/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors.doesnt_match([\"france\", \"germany\", \"dog\", \"japan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding models are capturing something meaningful about words and langauge in general, which can be seen from the word association tasks (e.g. man is to king as woman is to queen). We are also able to encode grammatical information (e.g. walk is to walked as swim is to swam). Hence, there is some kind of structural information in the language that the word embedding model is able to capture in terms of semantic relationships and co-occurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your own models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your own model with gensim is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load data with pandas__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"..\", \"data\", \"labelled_data\", \"fake_or_real_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize with ```spaCy```__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6335/6335 [15:07<00:00,  6.98it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for post in tqdm(data[\"text\"]):\n",
    "    # create a temporary list\n",
    "    tmp_list = []\n",
    "    # create spaCy doc object\n",
    "    doc = nlp(post.lower()) # we need everything to be lowercase to not consider the same word as different words just because of casing\n",
    "    # loop over\n",
    "    for token in doc:\n",
    "        tmp_list.append(token.text)\n",
    "    # append tmp_list to sentences\n",
    "    sentences.append(tmp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train model with ```gensim```__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences,  # input data\n",
    "                 size=50,              # embedding size (the number of dimensions)\n",
    "                 window=3,             # context window (the number of words before and after the target word)\n",
    "                 sg=1,                 # cbow or skip-gram (cbow=0, sg=1)\n",
    "                 negative=5,           # number of negative samples - the more negative samples, the longer the training will take, because it is going to classify more labels\n",
    "                 min_count=3,          # remove rare words. Words that appear less than 3 times will be excluded.\n",
    "                 workers=6)            # number of CPU processes/cores. More cores allows for more parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect most similar word__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.9361744523048401),\n",
       " ('administration', 0.8315683603286743),\n",
       " ('president', 0.7966960668563843),\n",
       " ('biden', 0.7546178102493286),\n",
       " ('rouhani', 0.7359713912010193),\n",
       " ('legacy', 0.7211123704910278),\n",
       " ('reclassify', 0.7160322666168213),\n",
       " ('congress', 0.7083786129951477),\n",
       " ('clouded', 0.7081670761108398),\n",
       " ('hardball', 0.6991458535194397)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('obama', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the problem with word embedding models is that they are data-hungry - it takes a lot of data to create useful representation of data. This is something we often run into when we are training our own models. Hence, when training your own model we need to consider whether we have enough data, and how we perform the preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7994694"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('jesus', 'god')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang101",
   "language": "python",
   "name": "lang101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
