{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initializing spaCy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model for English\n",
    "# In order to do so we create a spacy object called \"nlp\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Now we have a spacy object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)\n",
    "# We can see that it is a spacy object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform NLP tasks. We can use a spacy object to analyze text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Doc object\n",
    "# We annotate a random string using the spacy object\n",
    "doc = nlp(\"Ferocious winter weather sweeping across large parts of the central and southern US has brought record-breaking cold temperatures, left millions without power and killed at least 21 people across multiple states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)\n",
    "# It is a spacy object that comprises tokens\n",
    "# doc is a completely tokenized and annotated string, because the nlp() function has already been called on it - hence, the tokenization has already happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious winter weather sweeping across large parts of the central and southern US has brought record-breaking cold temperatures, left millions without power and killed at least 21 people across multiple states.\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "# The doc has already been tokenized in the background. This is cool about spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over the string using spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokens__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call many different methods/attributes on tokens (e.g. token.text is one function that can be called). SpaCy provides an overview of the different attributes/methods that can be called on a doc/token object: https://spacy.io/api/doc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious\n",
      "winter\n",
      "weather\n",
      "sweeping\n",
      "across\n",
      "large\n",
      "parts\n",
      "of\n",
      "the\n",
      "central\n",
      "and\n",
      "southern\n",
      "US\n",
      "has\n",
      "brought\n",
      "record\n",
      "-\n",
      "breaking\n",
      "cold\n",
      "temperatures\n",
      ",\n",
      "left\n",
      "millions\n",
      "without\n",
      "power\n",
      "and\n",
      "killed\n",
      "at\n",
      "least\n",
      "21\n",
      "people\n",
      "across\n",
      "multiple\n",
      "states\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# We iterate over each token in the string\n",
    "for token in doc:\n",
    "    print(token.text) # here we use the method .text \n",
    "# Now we can see that the string has been tokenized\n",
    "# Punctuations count as individual tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious 7398913484123842627\n",
      "winter 8844163100600735019\n",
      "weather 1756699799731398535\n",
      "sweeping 5287345485016755302\n",
      "across 12865022372469924430\n",
      "large 2751841902330220293\n",
      "parts 4485934323942657167\n",
      "of 886050111519832510\n",
      "the 7425985699627899538\n",
      "central 13919618042645247414\n",
      "and 2283656566040971221\n",
      "southern 12121605977752639731\n",
      "US 15397641858402276818\n",
      "has 14692702688101715474\n",
      "brought 3597906902382212429\n",
      "record 12677120423429974351\n",
      "- 9153284864653046197\n",
      "breaking 5527797886271786622\n",
      "cold 3117178197819627377\n",
      "temperatures 5627807717403523368\n",
      ", 2593208677638477497\n",
      "left 9707179535890930240\n",
      "millions 17365054503653917826\n",
      "without 4711265942760212190\n",
      "power 10405720708504167118\n",
      "and 2283656566040971221\n",
      "killed 3883960749573218104\n",
      "at 11667289587015813222\n",
      "least 12059514183285037132\n",
      "21 4686009691886217934\n",
      "people 7593739049417968140\n",
      "across 12865022372469924430\n",
      "multiple 16628341085578573424\n",
      "states 12763746643991857148\n",
      ". 12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma) # here we print both the token and the lemma\n",
    "# Now we get both the token and a number for each token. This is because spaCy converts every string into a number, because this makes opeartions make efficient. Each number is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious ferocious\n",
      "winter winter\n",
      "weather weather\n",
      "sweeping sweep\n",
      "across across\n",
      "large large\n",
      "parts part\n",
      "of of\n",
      "the the\n",
      "central central\n",
      "and and\n",
      "southern southern\n",
      "US US\n",
      "has have\n",
      "brought bring\n",
      "record record\n",
      "- -\n",
      "breaking break\n",
      "cold cold\n",
      "temperatures temperature\n",
      ", ,\n",
      "left leave\n",
      "millions million\n",
      "without without\n",
      "power power\n",
      "and and\n",
      "killed kill\n",
      "at at\n",
      "least least\n",
      "21 21\n",
      "people people\n",
      "across across\n",
      "multiple multiple\n",
      "states state\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_) #_ means that we want the string of the lemma\n",
    "# Now instead of getting the number for each token, we get the lemma itself as a string. We can see what happens with each word when it is lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious False\n",
      "winter False\n",
      "weather False\n",
      "sweeping False\n",
      "across False\n",
      "large False\n",
      "parts False\n",
      "of False\n",
      "the False\n",
      "central False\n",
      "and False\n",
      "southern False\n",
      "US False\n",
      "has False\n",
      "brought False\n",
      "record False\n",
      "- True\n",
      "breaking False\n",
      "cold False\n",
      "temperatures False\n",
      ", True\n",
      "left False\n",
      "millions False\n",
      "without False\n",
      "power False\n",
      "and False\n",
      "killed False\n",
      "at False\n",
      "least False\n",
      "21 False\n",
      "people False\n",
      "across False\n",
      "multiple False\n",
      "states False\n",
      ". True\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.is_punct) # Here we use the .is_punct method. This is just another example of a method that can be called on a token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferocious ADJ JJ\n",
      "winter NOUN NN\n",
      "weather NOUN NN\n",
      "sweeping VERB VBG\n",
      "across ADP IN\n",
      "large ADJ JJ\n",
      "parts NOUN NNS\n",
      "of ADP IN\n",
      "the DET DT\n",
      "central ADJ JJ\n",
      "and CCONJ CC\n",
      "southern ADJ JJ\n",
      "US PROPN NNP\n",
      "has AUX VBZ\n",
      "brought VERB VBN\n",
      "record NOUN NN\n",
      "- PUNCT HYPH\n",
      "breaking VERB VBG\n",
      "cold ADJ JJ\n",
      "temperatures NOUN NNS\n",
      ", PUNCT ,\n",
      "left VERB VBD\n",
      "millions NOUN NNS\n",
      "without ADP IN\n",
      "power NOUN NN\n",
      "and CCONJ CC\n",
      "killed VERB VBD\n",
      "at ADV RB\n",
      "least ADV RBS\n",
      "21 NUM CD\n",
      "people NOUN NNS\n",
      "across ADP IN\n",
      "multiple ADJ JJ\n",
      "states NOUN NNS\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_) # here we use the .pos and .tag to get the part-of-speech tag and the tag for each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^<br>\n",
    "UPOS = token.pos_ <br>\n",
    "fine-grained tags = token.tag_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang101",
   "language": "python",
   "name": "lang101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
