{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solution to assignment 2 - collocation extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm only going to be using ```re``` and ```string``` from the Python base libraries.\n",
    "\n",
    "I'll also be using ```numpy``` later for caculating a logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm only going to work with one text here; I won't go into how to iterate but it should be clear from the logic of the programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/100_english_novels/corpus/Dickens_Expectations_1861.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a quick tokenizer using ```regex``` which splits on non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick regex tokenizer for splitting files below\n",
    "def tokenize(input_string):\n",
    "    tokenizer = re.compile(r\"\\W+\")\n",
    "    return tokenizer.split(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then define my keyword and window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"love\"\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need all of our variables to calculate MI, only O11, R1, C1, and N.\n",
    "\n",
    "What are these variables? Well, in regular languge:\n",
    "\n",
    "- O11 = Number of times target word (u) appears with collocate (v) within the chosen window size\n",
    "- C1 = The number of times collocate appears across the whole text/corpus\n",
    "- R1 = The number of times target word (u) appears with any collocate within a chosen window size <br>\n",
    "\n",
    "So:\n",
    "- R1 = total number of all collocates with u\n",
    "- C1 = total occurrence of collocate v\n",
    "- N = total length of text/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize text__\n",
    "\n",
    "I'm first going to tokenize the text, make it all lowercase, remove all punctuation, and create a list of tokens.\n",
    "\n",
    "Why do I do these steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... tokenize the text and remove punctuation\n",
    "tokenized_text = []\n",
    "\n",
    "for word in tokenize(text):\n",
    "    # Lowercase\n",
    "    lowercase = word.lower()\n",
    "    # cleanup punctuation etc\n",
    "    cleaned = lowercase.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokenized_text.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then create a new list called ```tmp``` (for temporary). And then for every token in the list of tokenized text, I check if that token is equal to my keyword. \n",
    "\n",
    "In other words, for any token, is that token the word \"love\"?\n",
    "\n",
    "If so, I take 5 words before my target, and 5 words after and append these to my list called ```tmp```.\n",
    "\n",
    "(The logic here is the same as the KWIC function we made in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary list\n",
    "tmp = []\n",
    "# For the target word... \n",
    "for idx,word in enumerate(tokenized_text):\n",
    "    # If it's the keyword...\n",
    "    if word == keyword:\n",
    "        # Left context catch start of list\n",
    "        left_context = max(0, idx-window_size)\n",
    "        right_context = idx+window_size\n",
    "        # ... extract all words Â± 5 and add to tmp list.\n",
    "        full_context = tokenized_text[left_context:idx] + tokenized_text[idx+1:right_context]\n",
    "        # append to list\n",
    "        tmp.append(full_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of lists, where each sublist is the context around each specific instance of our target word.\n",
    "\n",
    "We want to flatten this into one big list of words. So for each sublist in our list ```tmp```, we append each token in the sublist to the variable ```flattened_list```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten list\n",
    "flattened_list = []\n",
    "# For each sublist in list of lists\n",
    "for sublist in tmp:\n",
    "    # For each item in sublist\n",
    "    for item in sublist:\n",
    "        # append\n",
    "        flattened_list.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to count how often each word appears in the ```flattened_list```. In other words, how often does each word appear in the context of our keyword?\n",
    "\n",
    "To do this, we create a ```set()``` of words from the flattened list and the use ```.count()``` how often each word in the set appears in ```flattened_list```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of collocate counts\n",
    "collocate_counts = []\n",
    "\n",
    "# for every collocate \n",
    "for word in set(flattened_list):\n",
    "    # Count how often each word appears as a collocate\n",
    "    count = flattened_list.count(word)\n",
    "    # Append tuple of word and count to list\n",
    "    collocate_counts.append((word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate some of our values for the MI formula.\n",
    "\n",
    "We can calulate R1 because that is how often any collocate occurs with our keyword.\n",
    "\n",
    "Hence, R1 is just the length of the ```flattened_list``` above! Similarly, N is equal to the size of the full text, or the lenght of ```tokenized_text``` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1 is how often u appears with any v at all\n",
    "R1 = len(flattened_list)\n",
    "\n",
    "# N is the length of the document\n",
    "N = len(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the rest of the values individually for each collocate.\n",
    "\n",
    "```collocate_counts``` contains tuples of ```(collocate, count)``` for every collocate for our keyword.\n",
    "\n",
    "Hence, the ```count``` value from the tuple is how often the collocate occurs with our keyword, which is how we defined O11!\n",
    "\n",
    "Lastly, C1 is a count of how often the collocate appears across the dataset in total, either with the keyword (O11) or without the keyword (O22)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values for calculating MI\n",
    "frequency_tuples = []\n",
    "\n",
    "# For every collocate\n",
    "for collocate, count in collocate_counts:\n",
    "    # O11 is how often it appears as collocate\n",
    "    O11 = count\n",
    "    # R1 is R1\n",
    "    R1 = R1\n",
    "    # N is N\n",
    "    N = N\n",
    "    # C1 is collocate distribution across the corpus\n",
    "    C1 = tokenized_text.count(collocate)\n",
    "    #\n",
    "    frequency_tuples.append((collocate, O11, R1, C1, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to calculate MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(O11, R1, C1, N):\n",
    "    # Calculate expected\n",
    "    E11 = (R1*C1)/N\n",
    "    # Calculate mutual information to 2 decimal places (actual value not very important)\n",
    "    mu = round(np.log2(O11/E11), 2)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for each tuple in ```frequency_tuples```, we can calculate the MI score.\n",
    "\n",
    "This ```for``` loop returns a list of tuples called mi_results, which shows the collocate, how often it occurs alongside our keyword, and the strenght of association between the keyword and collocate (MI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of outputs\n",
    "mi_results = []\n",
    "\n",
    "# each group of values in frequency_tuples\n",
    "for collocate, O11, R1, C1, N in frequency_tuples:\n",
    "    # Calculate MI using our function\n",
    "    mutual_information = MI(O11, R1, C1, N)\n",
    "    # append this to a list\n",
    "    mi_results.append((collocate, O11, mutual_information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('revenge', 1, 5.64),\n",
       " ('bright', 1, 3.59),\n",
       " ('beautiful', 1, 3.75),\n",
       " ('any', 1, 0.38),\n",
       " ('these', 1, 1.31),\n",
       " ('hated', 1, 6.87),\n",
       " ('hate', 1, 6.45),\n",
       " ('words', 3, 3.75),\n",
       " ('jealousy', 1, 5.28),\n",
       " ('in', 10, 0.21),\n",
       " ('cannot', 1, 3.36),\n",
       " ('very', 1, -0.19),\n",
       " ('commonly', 1, 7.45),\n",
       " ('neighbor', 1, 5.64),\n",
       " ('miss', 4, 1.87),\n",
       " ('sending', 1, 6.45),\n",
       " ('when', 1, -1.33),\n",
       " ('deeper', 3, 7.45),\n",
       " ('do', 1, -0.39),\n",
       " ('father', 1, 2.34)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_results[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner tabular output with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then read this list of tuples into ```pandas``` to allows us to filter and explore the results more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(mi_results, columns =[\"collocate\", \"freq\", \"MI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we order on two columns, showing the results in descending order for the column ```MI``` and descending order of ```freq``` for collocates with the same MI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocate</th>\n",
       "      <th>freq</th>\n",
       "      <th>MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>wounds</td>\n",
       "      <td>2</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>reputed</td>\n",
       "      <td>1</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>favors</td>\n",
       "      <td>2</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>charity</td>\n",
       "      <td>1</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>rigidity</td>\n",
       "      <td>1</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deeper</td>\n",
       "      <td>3</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>dire</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>disinterestedness</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>rear</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>generosity</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>gush</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>youthful</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>adore</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>commonly</td>\n",
       "      <td>1</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>love</td>\n",
       "      <td>28</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>religion</td>\n",
       "      <td>1</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>hundreds</td>\n",
       "      <td>1</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>mankind</td>\n",
       "      <td>1</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hated</td>\n",
       "      <td>1</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>haughty</td>\n",
       "      <td>1</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             collocate  freq    MI\n",
       "104             wounds     2  8.45\n",
       "120            reputed     1  8.45\n",
       "144             favors     2  8.45\n",
       "40             charity     1  8.45\n",
       "206           rigidity     1  8.45\n",
       "17              deeper     3  7.45\n",
       "71                dire     1  7.45\n",
       "163  disinterestedness     1  7.45\n",
       "59                rear     1  7.45\n",
       "41          generosity     1  7.45\n",
       "188               gush     1  7.45\n",
       "205           youthful     1  7.45\n",
       "169              adore     1  7.45\n",
       "12            commonly     1  7.45\n",
       "171               love    28  7.35\n",
       "101           religion     1  6.87\n",
       "60            hundreds     1  6.87\n",
       "131            mankind     1  6.87\n",
       "5                hated     1  6.87\n",
       "154            haughty     1  6.87"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sort_values([\"MI\", \"freq\"], ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
